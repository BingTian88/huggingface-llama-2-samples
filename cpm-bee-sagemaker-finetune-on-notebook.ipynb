{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f55389-9564-41e7-a8ae-0bbd0904ced3",
   "metadata": {},
   "source": [
    "# CPM-Bee 教程：基础微调\n",
    "\n",
    "本示例介绍 [CPM-Bee](https://github.com/OpenBMB/CPM-Bee) 的微调，CPM-Bee是一个基座模型，即从零开始通过预训练得来。本示例参考官方[基础微调文档](https://github.com/OpenBMB/CPM-Bee/tree/main/tutorials/basic_task_finetune)，使用 Amazon SageMaker Notebook 实例进行模型微调。\n",
    "\n",
    " - 本教程使用 **conda_mxnet_p38** 环境，您可以在右上角切换。\n",
    " - 本教程使用 **ml.g5.12xlarge** 实例类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6befb1e0-c766-4a52-b70a-f66f4fc03d98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/CPM/CPM-Bee-on-SageMaker\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1732236",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com, https://download.pytorch.org/whl/\n",
      "Collecting torch==1.11.0+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp38-cp38-linux_x86_64.whl (1637.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 GB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions (from torch==1.11.0+cu113)\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: typing-extensions, torch\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "black 23.3.0 requires packaging>=22.0, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.11.0+cu113 typing-extensions-4.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall torch==1.11.0+cu113 --extra-index-url https://download.pytorch.org/whl/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f8c18",
   "metadata": {},
   "source": [
    "### 下载 CPM-Bee 代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc932e98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CPM-Bee'...\n",
      "remote: Enumerating objects: 397, done.\u001b[K\n",
      "remote: Counting objects: 100% (196/196), done.\u001b[K\n",
      "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
      "remote: Total 397 (delta 78), reused 151 (delta 65), pack-reused 201\u001b[K\n",
      "Receiving objects: 100% (397/397), 6.74 MiB | 24.13 MiB/s, done.\n",
      "Resolving deltas: 100% (141/141), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/OpenBMB/CPM-Bee.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2fdae",
   "metadata": {},
   "source": [
    "#### 安装所需的依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f3d300e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: torch<2.0.0,>=1.10 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from -r CPM-Bee/src/requirements.txt (line 1)) (1.11.0+cu113)\n",
      "Collecting bmtrain>=0.2.1 (from -r CPM-Bee/src/requirements.txt (line 2))\n",
      "  Using cached bmtrain-0.2.2.tar.gz (58 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jieba (from -r CPM-Bee/src/requirements.txt (line 3))\n",
      "  Using cached jieba-0.42.1.tar.gz (19.2 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from -r CPM-Bee/src/requirements.txt (line 4)) (4.63.2)\n",
      "Collecting tensorboard (from -r CPM-Bee/src/requirements.txt (line 5))\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from -r CPM-Bee/src/requirements.txt (line 6)) (1.22.4)\n",
      "Collecting spacy (from -r CPM-Bee/src/requirements.txt (line 7))\n",
      "  Using cached spacy-3.6.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
      "Collecting opendelta (from -r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached opendelta-0.3.2-py3-none-any.whl (88 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from torch<2.0.0,>=1.10->-r CPM-Bee/src/requirements.txt (line 1)) (4.7.1)\n",
      "Collecting absl-py>=0.4 (from tensorboard->-r CPM-Bee/src/requirements.txt (line 5))\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard->-r CPM-Bee/src/requirements.txt (line 5))\n",
      "  Using cached grpcio-1.56.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard->-r CPM-Bee/src/requirements.txt (line 5))\n",
      "  Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard->-r CPM-Bee/src/requirements.txt (line 5))\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard->-r CPM-Bee/src/requirements.txt (line 5))\n",
      "  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from tensorboard->-r CPM-Bee/src/requirements.txt (line 5)) (3.20.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from tensorboard->-r CPM-Bee/src/requirements.txt (line 5)) (2.29.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from tensorboard->-r CPM-Bee/src/requirements.txt (line 5)) (67.7.2)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r CPM-Bee/src/requirements.txt (line 5))\n",
      "  Using cached tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from tensorboard->-r CPM-Bee/src/requirements.txt (line 5)) (2.3.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from tensorboard->-r CPM-Bee/src/requirements.txt (line 5)) (0.40.0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy->-r CPM-Bee/src/requirements.txt (line 7))\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy->-r CPM-Bee/src/requirements.txt (line 7))\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy->-r CPM-Bee/src/requirements.txt (line 7))\n",
      "  Using cached murmurhash-1.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy->-r CPM-Bee/src/requirements.txt (line 7))\n",
      "  Using cached cymem-2.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy->-r CPM-Bee/src/requirements.txt (line 7))\n",
      "  Using cached preshed-3.0.8-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8 (from spacy->-r CPM-Bee/src/requirements.txt (line 7))\n",
      "  Using cached thinc-8.1.10-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (928 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy->-r CPM-Bee/src/requirements.txt (line 7))\n",
      "  Using cached wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy->-r CPM-Bee/src/requirements.txt (line 7))\n",
      "  Using cached srsly-2.4.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy->-r CPM-Bee/src/requirements.txt (line 7))\n",
      "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy->-r CPM-Bee/src/requirements.txt (line 7))\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting pathy>=0.10.0 (from spacy->-r CPM-Bee/src/requirements.txt (line 7))\n",
      "  Using cached pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy->-r CPM-Bee/src/requirements.txt (line 7))\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 (from spacy->-r CPM-Bee/src/requirements.txt (line 7))\n",
      "  Using cached pydantic-1.10.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from spacy->-r CPM-Bee/src/requirements.txt (line 7)) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from spacy->-r CPM-Bee/src/requirements.txt (line 7)) (21.3)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy->-r CPM-Bee/src/requirements.txt (line 7))\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting transformers>=4.10.0 (from opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "Collecting datasets>=1.17.0 (from opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached datasets-2.13.1-py3-none-any.whl (486 kB)\n",
      "Collecting sentencepiece>=0.1.96 (from opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (5.1.1)\n",
      "Collecting rich (from opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached rich-13.4.2-py3-none-any.whl (239 kB)\n",
      "Collecting web.py (from opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached web.py-0.62.tar.gz (623 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gitpython (from opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (1.8.1)\n",
      "Collecting sklearn (from opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached sklearn-0.0.post5.tar.gz (3.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting delta-center-client==0.0.4 (from opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached delta_center_client-0.0.4-py3-none-any.whl (8.5 kB)\n",
      "Collecting oss2==2.15.0 (from delta-center-client==0.0.4->opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached oss2-2.15.0.tar.gz (226 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting yacs>=0.1.6 (from delta-center-client==0.0.4->opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting crcmod>=1.7 (from oss2==2.15.0->delta-center-client==0.0.4->opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached crcmod-1.7.tar.gz (89 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pycryptodome>=3.4.7 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from oss2==2.15.0->delta-center-client==0.0.4->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (3.16.0)\n",
      "Collecting aliyun-python-sdk-kms>=2.4.1 (from oss2==2.15.0->delta-center-client==0.0.4->opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached aliyun_python_sdk_kms-2.16.1-py2.py3-none-any.whl (70 kB)\n",
      "Collecting aliyun-python-sdk-core>=2.13.12 (from oss2==2.15.0->delta-center-client==0.0.4->opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached aliyun-python-sdk-core-2.13.36.tar.gz (440 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from oss2==2.15.0->delta-center-client==0.0.4->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from datasets>=1.17.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from datasets>=1.17.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from datasets>=1.17.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (1.4.4)\n",
      "Collecting xxhash (from datasets>=1.17.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from datasets>=1.17.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from datasets>=1.17.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from datasets>=1.17.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (3.8.4)\n",
      "Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets>=1.17.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from datasets>=1.17.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (5.4.1)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard->-r CPM-Bee/src/requirements.txt (line 5))\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard->-r CPM-Bee/src/requirements.txt (line 5))\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r CPM-Bee/src/requirements.txt (line 5)) (4.7.2)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r CPM-Bee/src/requirements.txt (line 5)) (1.26.14)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r CPM-Bee/src/requirements.txt (line 5))\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard->-r CPM-Bee/src/requirements.txt (line 5)) (4.13.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from packaging>=20.0->spacy->-r CPM-Bee/src/requirements.txt (line 7)) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r CPM-Bee/src/requirements.txt (line 5)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r CPM-Bee/src/requirements.txt (line 5)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r CPM-Bee/src/requirements.txt (line 5)) (2023.5.7)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.2.0,>=8.1.8->spacy->-r CPM-Bee/src/requirements.txt (line 7))\n",
      "  Using cached blis-0.7.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.2.0,>=8.1.8->spacy->-r CPM-Bee/src/requirements.txt (line 7))\n",
      "  Using cached confection-0.1.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from transformers>=4.10.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from transformers>=4.10.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (2023.5.5)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.10.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Collecting safetensors>=0.3.1 (from transformers>=4.10.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached safetensors-0.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy->-r CPM-Bee/src/requirements.txt (line 7)) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard->-r CPM-Bee/src/requirements.txt (line 5)) (2.1.2)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython->opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from rich->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (2.15.1)\n",
      "Collecting cheroot (from web.py->opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached cheroot-10.0.0-py3-none-any.whl (101 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from aiohttp->datasets>=1.17.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from aiohttp->datasets>=1.17.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from aiohttp->datasets>=1.17.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from aiohttp->datasets>=1.17.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (1.9.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from aiohttp->datasets>=1.17.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from aiohttp->datasets>=1.17.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (1.3.1)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r CPM-Bee/src/requirements.txt (line 5)) (3.15.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r CPM-Bee/src/requirements.txt (line 5)) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r CPM-Bee/src/requirements.txt (line 5))\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: more-itertools>=2.6 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from cheroot->web.py->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (9.1.0)\n",
      "Collecting jaraco.functools (from cheroot->web.py->opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached jaraco.functools-3.8.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from pandas->datasets>=1.17.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from pandas->datasets>=1.17.0->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (2023.3)\n",
      "Collecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2==2.15.0->delta-center-client==0.0.4->opendelta->-r CPM-Bee/src/requirements.txt (line 8))\n",
      "  Using cached jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: cryptography>=2.6.0 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2==2.15.0->delta-center-client==0.0.4->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (40.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2==2.15.0->delta-center-client==0.0.4->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2==2.15.0->delta-center-client==0.0.4->opendelta->-r CPM-Bee/src/requirements.txt (line 8)) (2.21)\n",
      "Building wheels for collected packages: bmtrain, jieba, oss2, sklearn, web.py, aliyun-python-sdk-core, crcmod\n",
      "  Building wheel for bmtrain (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bmtrain: filename=bmtrain-0.2.2-cp38-cp38-linux_x86_64.whl size=432111 sha256=515c845f05a6f2aa333b4c18b70632411c2ef6e219d3acad7f3345d5d61c04e6\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/1f/87/0f/9a36073e7de80c24a283b211816ce36371f776b5799f21c3b8\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=9962d08303bd1485ad46f50f3500ad6f4ec54f5f6287757836d8d68375f700f7\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/ca/38/d8/dfdfe73bec1d12026b30cb7ce8da650f3f0ea2cf155ea018ae\n",
      "  Building wheel for oss2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for oss2: filename=oss2-2.15.0-py3-none-any.whl size=103433 sha256=4d392196211ed4a9a94192f5a53c874fae5a4ce49d792089831dcd227b0ef33b\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/41/aa/57/f7a33bd9e7110547c9b7b24638c688b6fbcde9a9635f41020e\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0.post5-py3-none-any.whl size=2950 sha256=58c3f19e3ab7aa02efec29301b7feca355b48e5aa433aa2212a31edf5c7527f1\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/44/08/18/d0b86f591e929e063b3134b126c8a77b3758e527fe1a3f6fb8\n",
      "  Building wheel for web.py (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for web.py: filename=web.py-0.62-py3-none-any.whl size=78567 sha256=0da5efa227449d1db779eecca361a0d51648af7978a75e358698726eff18d14d\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/d7/b5/8c/d4b298304a41865a1814e79218f2a33963f735796bef785829\n",
      "  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.13.36-py3-none-any.whl size=533194 sha256=76bfe10f4f4f2a42614b593b0bb746561b88f6cb23c29cae3e07bcd7cb408083\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/50/51/7d/304d0c92509363ab568526b81e53573e2e1e249ce35d492ea4\n",
      "  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp38-cp38-linux_x86_64.whl size=23266 sha256=7c46028273b939562adfd34f1e7aa06f880fe9c8cccd8d27cb71e84057f2a286\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/ca/5a/02/f3acf982a026f3319fb3e798a8dca2d48fafee7761788562e9\n",
      "Successfully built bmtrain jieba oss2 sklearn web.py aliyun-python-sdk-core crcmod\n",
      "Installing collected packages: tokenizers, sklearn, sentencepiece, safetensors, jieba, cymem, crcmod, yacs, xxhash, wasabi, typer, tensorboard-data-server, spacy-loggers, spacy-legacy, smmap, smart-open, pydantic, pyasn1-modules, oauthlib, murmurhash, mdurl, langcodes, jmespath, jaraco.functools, grpcio, catalogue, cachetools, bmtrain, blis, absl-py, srsly, requests-oauthlib, preshed, pathy, markdown-it-py, markdown, huggingface-hub, google-auth, gitdb, cheroot, web.py, transformers, rich, google-auth-oauthlib, gitpython, confection, aliyun-python-sdk-core, thinc, tensorboard, datasets, aliyun-python-sdk-kms, spacy, oss2, delta-center-client, opendelta\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 1.0.1\n",
      "    Uninstalling jmespath-1.0.1:\n",
      "      Successfully uninstalled jmespath-1.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.167.0 requires PyYAML==6.0, but you have pyyaml 5.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-1.4.0 aliyun-python-sdk-core-2.13.36 aliyun-python-sdk-kms-2.16.1 blis-0.7.9 bmtrain-0.2.2 cachetools-5.3.1 catalogue-2.0.8 cheroot-10.0.0 confection-0.1.0 crcmod-1.7 cymem-2.0.7 datasets-2.13.1 delta-center-client-0.0.4 gitdb-4.0.10 gitpython-3.1.32 google-auth-2.22.0 google-auth-oauthlib-1.0.0 grpcio-1.56.0 huggingface-hub-0.16.4 jaraco.functools-3.8.0 jieba-0.42.1 jmespath-0.10.0 langcodes-3.3.0 markdown-3.4.3 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.9 oauthlib-3.2.2 opendelta-0.3.2 oss2-2.15.0 pathy-0.10.2 preshed-3.0.8 pyasn1-modules-0.3.0 pydantic-1.10.11 requests-oauthlib-1.3.1 rich-13.4.2 safetensors-0.3.1 sentencepiece-0.1.99 sklearn-0.0.post5 smart-open-6.3.0 smmap-5.0.0 spacy-3.6.0 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.7 tensorboard-2.13.0 tensorboard-data-server-0.7.1 thinc-8.1.10 tokenizers-0.13.3 transformers-4.30.2 typer-0.9.0 wasabi-1.1.2 web.py-0.62 xxhash-3.2.0 yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "#%%script bash\n",
    "!export TORCH_CUDA_ARCH_LIST=\"6.0 6.1 7.0 7.5 8.0 8.6+PTX\"\n",
    "!export SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True \n",
    "!pip install -r CPM-Bee/src/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd34a44-77f7-464c-bd98-4147deac0c56",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CPM-Bee 数据格式介绍\n",
    "\n",
    "CPM-Bee基座模型可以将多种自然语言处理任务统一用生成的方式解决。CPM-Bee 采用特殊的多任务预训练模式，所有的数据都统一用一个字典来管理。我们可以任意设计字典中的键值对来表达我们希望模型做的事情，同时预留一个字段，用于存储模型给出的答案。注意，字段是必需的，基本格式如下：\n",
    "\n",
    "```json \n",
    "{\"some_key\": \"...\", \"<ans>\": \"\"}\n",
    "```\n",
    "\n",
    "尽管输入数据的格式是任意的，但由于模型在预训练阶段使用了有限的几种数据格式，我们建议您在使用CPM-Bee推理时尽量使用这些参考格式。\n",
    "\n",
    "文本生成\n",
    "```json\n",
    "# 文本生成\n",
    "{\"input\": \"今天天气不错，\", \"prompt\":\"往后写100字\", \"<ans>\":\"\"}\n",
    "```\n",
    "`input`字段用于填写上下文，它并不是唯一的，您可以使用\"source\", \"document\", \"query\", \"text\", \"文章\", \"文档\", \"原文\", \"输入\", \"context\", \"上下文\"等类似的键来替换。\n",
    "\n",
    "`prompt`字段用来给出一些提示和指定任务，该字段并不是必需的，但是我们建议您使用合理的 `prompt` 来更好地驱动模型。`prompt`也可以被\"hint\", \"task\", \"prompt\", \"任务\", \"提示\", \"目标\", \"target\"等替换。请注意，prompt 一般会提供一些控制信息，如\"往后写xxx字\"，\"中翻英\"，\"给这段话生成摘要\"等。\n",
    "\n",
    "翻译\n",
    "\n",
    "```json\n",
    "# 翻译\n",
    "{\"input\": \"今天天气不错，\", \"prompt\":\"中翻英\", \"<ans>\":\"\"}\n",
    "```\n",
    "\n",
    "CPM-Bee目前支持中英互译。`prompt`一般可选\"中翻英\"/\"英翻中\"，\"中译英\"/\"英译中\"，\"把文章翻译为英文\"/\"把文章翻译为中文\"，\"Translate from English to Chinese\"等。\n",
    "\n",
    "问答\n",
    "```json\n",
    "# 问答\n",
    "{\"input\": \"今天天气不错，\", \"prompt\":\"问答\", \"question\": \"今天天气怎么样\", \"<ans>\":\"\"}\n",
    "```\n",
    "\n",
    "选择题\n",
    "\n",
    "```json\n",
    "# 选择题\n",
    "{\"input\": \"今天天气不错，\", \"prompt\":\"选择题\", \"question\": \"今天天气怎么样\", \"options\": {\"<option_0>\": \"好\", \"<option_1>\": \"坏\"}, \"<ans>\":\"\"}\n",
    "```\n",
    "\n",
    "`options`可以等价替换为\"answers\", \"candidates\", \"选项\"...\n",
    "\n",
    "命名实体识别\n",
    "\n",
    "```json\n",
    "# NER\n",
    "{\"input\":\"在司法部工作的小楠说，今天北京天气不错\",\"<ans>\":{\"人名\":\"\",\"地名\":\"\",\"机构名\": \"\"}}\n",
    "```\n",
    "\n",
    "以上是一些常见的任务的数据格式。请注意里面用到的字段不是严格限定的，您可以做一些近似语义的替换，比如把\"中翻英\"替换为\"把这段话翻译成英文\"。您也可以在微调时自由设计数据格式，例如，当您希望微调一个对话模型，您可以构造数据格式为\n",
    "\n",
    "```json\n",
    "{\"input\": \"用户： 你好，我想问一下明天天气会怎样？\\n<sep>AI： 你好！明天的天气会根据你所在的城市而异，请告诉我你所在的城市。\\n<sep>用户： 我在北京。\\n<sep>AI：\", \"<ans>\": \" 明天北京天气预计为阴转多云，最高气温26℃，最低气温18℃。\"}\n",
    "```\n",
    "\n",
    "您也可以不使用`<sep>`，使用如下格式也可以：\n",
    "\n",
    "```json\n",
    "{\"input\": \"<问题>你好，我想问一下明天天气会怎样？\\n<答案>你好！明天的天气会根据你所在的城市而异，请告诉我你所在的城市。\\n<问题>我在北京。\\n<答案>\", \"<ans>\": \" 明天北京天气预计为阴转多云，最高气温26℃，最低气温18℃。\"}\n",
    "```\n",
    "总之，您可以灵活定义您的数据格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d3b58",
   "metadata": {},
   "source": [
    "### 使用CPM-Bee进行基础任务微调\n",
    "\n",
    "本教程将以一个序列-序列任务为例介绍对 CPM-Bee 基座模型的微调。这里我们选择的任务需要将一句白话文“翻译”成一句古诗。首先，微调需要准备原始数据，格式如下：\n",
    "```json\n",
    "{\"target\": \"3\", \"input\": \"[翻译]昏暗的灯熄灭了又被重新点亮。[0]渔灯灭复明[1]残灯灭又然[2]残灯暗复明[3]残灯灭又明[答案]\"}\n",
    "```\n",
    "放置在路径`raw_data/`下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3c8cf",
   "metadata": {},
   "source": [
    "官方示例中已经提供了该 raw data，并放置在 CPM-Bee/tutorials/basic_task_finetune/raw_data 下面，本教程将直接使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798957e",
   "metadata": {},
   "source": [
    "### 下载基础模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "169d5ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf CPM-Bee/src/10B-ckpts/\n",
    "!mkdir CPM-Bee/src/10B-ckpts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3640b597-020f-4457-acbd-61aa78727567",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: huggingface_hub in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (0.16.4)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from huggingface_hub) (3.6.0)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from huggingface_hub) (2023.5.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from huggingface_hub) (2.29.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from huggingface_hub) (4.63.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from huggingface_hub) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from huggingface_hub) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from requests->huggingface_hub) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/mxnet_p38/lib/python3.8/site-packages (from requests->huggingface_hub) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56494b4b-f152-4980-9705-f4eb1a713585",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00768589973449707,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Fetching 5 files",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1e8e420aed46fab7b1eb3c6bd30105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003605365753173828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)neration_config.json",
       "rate": null,
       "total": 275,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6766edcbff4f118dca140890ac93fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/275 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007527589797973633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)okenizer_config.json",
       "rate": null,
       "total": 217,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ceae43dc7d4b6db0e5678726a417eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/217 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006965160369873047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)61288937/config.json",
       "rate": null,
       "total": 661,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0ebc375e9c49739d85a06237794c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)61288937/config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01500391960144043,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)3361288937/vocab.txt",
       "rate": null,
       "total": 629411,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a926861383cf4e7a838356e628e15bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)3361288937/vocab.txt:   0%|          | 0.00/629k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0040416717529296875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 19232366411,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c712a2e37d94f0aa177af7120512e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/19.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/CPM/CPM-Bee-on-SageMaker/CPM-Bee/src/10B-ckpts'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(repo_id=\"openbmb/cpm-bee-10b\", local_dir = 'CPM-Bee/src/10B-ckpts/', \n",
    "                  allow_patterns=[\"*.bin\", \"*.json\", \"*.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5478284a",
   "metadata": {},
   "source": [
    "#### 重新调整数据格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9056869",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd CPM-Bee/tutorials/basic_task_finetune && python data_reformat.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1e6b2e",
   "metadata": {},
   "source": [
    "得到格式：\n",
    "```json\n",
    "{\"input\": \"昏暗的灯熄灭了又被重新点亮。\", \"options\": {\"<option_0>\": \"渔灯灭复明\", \"<option_1>\": \"残灯灭又然\", \"<option_2>\": \"残灯暗复明\", \"<option_3>\": \"残灯灭又明\"}, \"question\": \"这段话形容了哪句诗的意境？\", \"<ans>\": \"<option_3>\"}\n",
    "```\n",
    "放置在路径`CPM-Bee/tutorials/basic_task_finetune/bee_data/`下。\n",
    "\n",
    "注：该格式为参考格式。微调时，您可以自由设计您的数据格式，可以不设置`prompt`字段，只要所提供的数据涵盖所有必要信息即可，但我们一般推荐将输入文本字段标识为\"input\"/\"document\"/\"doc\"，如果是选择题，则应当添加\"options\"字段与\"question\"字段；如果是一般的文本生成，包含`input`+`\\<ans\\>`即可。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd28f1",
   "metadata": {},
   "source": [
    "#### 构建二进制数据文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1834c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf CPM-Bee/tutorials/basic_task_finetune/bin_data\n",
    "!mkdir CPM-Bee/tutorials/basic_task_finetune/bin_data\n",
    "!rm -rf tmp\n",
    "!rm -rf CPM-Bee/tutorials/basic_task_finetune/bee_data/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09f4e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_path = \"CPM-Bee/tutorials/basic_task_finetune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce1cc750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CPM-Bee/tutorials/basic_task_finetune/bee_data/train.jsonl: 100%|██████████| 21778/21778 [00:00<00:00, 132097.85it/s]\n",
      "Shuffle step 1/2: 100%|██████████| 21778/21778 [00:00<00:00, 267009.90it/s]\n",
      "Shuffle step 2/2: 100%|██████████| 1/1 [00:00<00:00, 19.33it/s]\n",
      "CPM-Bee/tutorials/basic_task_finetune/bee_data/eval.jsonl: 100%|██████████| 2720/2720 [00:00<00:00, 131945.17it/s]\n",
      "Shuffle step 1/2: 100%|██████████| 2720/2720 [00:00<00:00, 260747.99it/s]\n",
      "Shuffle step 2/2: 100%|██████████| 1/1 [00:00<00:00, 155.29it/s]\n"
     ]
    }
   ],
   "source": [
    "%%script env finetune_path=$finetune_path bash\n",
    "python CPM-Bee/src/preprocess_dataset.py --input ${finetune_path}/bee_data --output_path ${finetune_path}/bin_data --output_name ccpm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098475f9",
   "metadata": {},
   "source": [
    "生成的数据将放在路径`bin_data/`下。\n",
    "\n",
    "**注：**应确保没有同名路径`ccpm_example/bin_data/`，如存在同名路径，应先删除该路径再运行上述指令。如未提前删除，该指令会报错`ValueError: Dataset name exists`，同时产生一个新路径`tmp/`，此时应当连同`tmp/`与同名路径`ccpm_example/bin_data/`一并删除，之后再运行上述指令即可。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29429dfe-5a3f-4a95-83c6-fefd48730a57",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896c14c4-ee95-4b57-9441-7d7efaedd441",
   "metadata": {},
   "source": [
    "运行以下指令进行训练："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7efda05",
   "metadata": {},
   "source": [
    "修改模型微调脚本`CPM-Bee/src/scripts/finetune_cpm_bee.sh`为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3030bc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting CPM-Bee/src/scripts/finetune_cpm_bee.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile CPM-Bee/src/scripts/finetune_cpm_bee.sh\n",
    "#! /bin/bash\n",
    "\n",
    "# 四卡微调\n",
    "export CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "GPUS_PER_NODE=4\n",
    "\n",
    "NNODES=1\n",
    "MASTER_ADDR=\"localhost\"\n",
    "MASTER_PORT=12346\n",
    "\n",
    "OPTS=\"\"\n",
    "OPTS+=\" --use-delta\"  # 使用增量微调（delta-tuning）\n",
    "OPTS+=\" --model-config CPM-Bee/src/config/cpm-bee-10b.json\"  # 模型配置文件\n",
    "OPTS+=\" --dataset CPM-Bee/tutorials/basic_task_finetune/bin_data/train\"  # 训练集路径\n",
    "OPTS+=\" --eval_dataset CPM-Bee/tutorials/basic_task_finetune/bin_data/eval\"  # 验证集路径\n",
    "OPTS+=\" --epoch 1\"  # 训练epoch数\n",
    "OPTS+=\" --batch-size 3\"    # 数据批次大小\n",
    "OPTS+=\" --train-iters 100\"  # 用于lr_schedular\n",
    "OPTS+=\" --save-name cpm_bee_finetune\"  # 保存名称\n",
    "OPTS+=\" --max-length 2048\" # 最大长度\n",
    "OPTS+=\" --save results/\"  # 保存路径\n",
    "OPTS+=\" --lr 0.0001\"    # 学习率\n",
    "OPTS+=\" --inspect-iters 100\"  # 每100个step进行一次检查(bmtrain inspect)\n",
    "OPTS+=\" --warmup-iters 1\" # 预热学习率的步数为1\n",
    "OPTS+=\" --eval-interval 50\"  # 每50步验证一次\n",
    "OPTS+=\" --early-stop-patience 5\"  # 如果验证集loss连续5次不降，停止微调\n",
    "OPTS+=\" --lr-decay-style noam\"  # 选择noam方式调度学习率\n",
    "OPTS+=\" --weight-decay 0.01\"  # 优化器权重衰减率为0.01\n",
    "OPTS+=\" --clip-grad 1.0\"  # 半精度训练的grad clip\n",
    "OPTS+=\" --loss-scale 32768\"  # 半精度训练的loss scale\n",
    "OPTS+=\" --start-step 0\"  # 用于加载lr_schedular的中间状态\n",
    "OPTS+=\" --load CPM-Bee/src/10B-ckpts/pytorch_model.bin\"  # 模型参数文件\n",
    "\n",
    "\n",
    "CMD=\"torchrun --nnodes=${NNODES} --nproc_per_node=${GPUS_PER_NODE} --rdzv_id=1 --rdzv_backend=c10d --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} CPM-Bee/src/finetune_cpm_bee.py ${OPTS}\"\n",
    "\n",
    "echo ${CMD}\n",
    "$CMD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86f2648-36d1-4ca6-9a19-ba354ebe6c8d",
   "metadata": {},
   "source": [
    "## 开启训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c4e415",
   "metadata": {
    "tags": []
   },
   "source": [
    "直接运行脚本即可开始微调："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37813667",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchrun --nnodes=1 --nproc_per_node=4 --rdzv_id=1 --rdzv_backend=c10d --rdzv_endpoint=localhost:12346 CPM-Bee/src/finetune_cpm_bee.py --use-delta --model-config CPM-Bee/src/config/cpm-bee-10b.json --dataset CPM-Bee/tutorials/basic_task_finetune/bin_data/train --eval_dataset CPM-Bee/tutorials/basic_task_finetune/bin_data/eval --epoch 1 --batch-size 3 --train-iters 100 --save-name cpm_bee_finetune --max-length 2048 --save results/ --lr 0.0001 --inspect-iters 100 --warmup-iters 1 --eval-interval 50 --early-stop-patience 5 --lr-decay-style noam --weight-decay 0.01 --clip-grad 1.0 --loss-scale 32768 --start-step 0 --load CPM-Bee/src/10B-ckpts/pytorch_model.bin\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "====================== Initialization ======================\n",
      "rank :          0\n",
      "local_rank :    0\n",
      "world_size :    4\n",
      "local_size :    4\n",
      "master :        ip-172-16-181-136.ec2.internal:47927\n",
      "device :        0\n",
      "cpus :          [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "\n",
      "====================== Initialization ======================\n",
      "rank :          1\n",
      "local_rank :    1\n",
      "world_size :    4\n",
      "local_size :    4\n",
      "master :        ip-172-16-181-136.ec2.internal:47927\n",
      "device :        1\n",
      "cpus :          [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
      "                 23]\n",
      "\n",
      "====================== Initialization ======================\n",
      "rank :          2\n",
      "local_rank :    2\n",
      "world_size :    4\n",
      "local_size :    4\n",
      "master :        ip-172-16-181-136.ec2.internal:47927\n",
      "device :        2\n",
      "cpus :          [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
      "                 35]\n",
      "\n",
      "====================== Initialization ======================\n",
      "rank :          3\n",
      "local_rank :    3\n",
      "world_size :    4\n",
      "local_size :    4\n",
      "master :        ip-172-16-181-136.ec2.internal:47927\n",
      "device :        3\n",
      "cpus :          [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46,\n",
      "                 47]\n",
      "\n",
      "\u001b[37mroot\u001b[0m\n",
      "├── \u001b[37mencoder \u001b[0m\u001b[32m(Encoder)\u001b[0m\n",
      "│   ├── \u001b[37mlayers \u001b[0m\u001b[32m(TransformerBlockList)\u001b[0m\n",
      "│   │   └── \u001b[31m0-47\u001b[0m\u001b[32m(CheckpointBlock)\u001b[0m\n",
      "│   │       ├── \u001b[37mself_att \u001b[0m\u001b[32m(SelfAttentionBlock)\u001b[0m\n",
      "│   │       │   ├── \u001b[37mlayernorm_before_attention \u001b[0m\u001b[32m(LayerNorm) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   │       │   └── \u001b[37mself_attention \u001b[0m\u001b[32m(Attention)\u001b[0m\n",
      "│   │       │       ├── \u001b[31mproject_q,project_v\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   │       │       │   └── \u001b[37mlora \u001b[0m\u001b[32m(DistributedLowRankLinear) \u001b[0m\u001b[35mlora_A:[8192] \u001b[0m\n",
      "│   │       │       │       \u001b[35mlora_B:[8192]\u001b[0m\n",
      "│   │       │       └── \u001b[31mproject_k,attention_out\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   │       └── \u001b[37mffn \u001b[0m\u001b[32m(FFNBlock)\u001b[0m\n",
      "│   │           ├── \u001b[37mlayernorm_before_ffn \u001b[0m\u001b[32m(LayerNorm) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   │           └── \u001b[37mffn \u001b[0m\u001b[32m(FeedForward)\u001b[0m\n",
      "│   │               ├── \u001b[37mw_in \u001b[0m\u001b[32m(DenseGatedACT)\u001b[0m\n",
      "│   │               │   ├── \u001b[37mw_0 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[12587008]\u001b[0m\n",
      "│   │               │   └── \u001b[37mw_1 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[35649536]\u001b[0m\n",
      "│   │               └── \u001b[37mw_out \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   └── \u001b[37moutput_layernorm \u001b[0m\u001b[32m(LayerNorm) \u001b[0m\u001b[90mweight:[1024]\u001b[0m\n",
      "├── \u001b[37minput_embedding \u001b[0m\u001b[32m(EmbeddingExt) \u001b[0m\u001b[90mweight:[88660992]\u001b[0m\n",
      "└── \u001b[37mposition_bias \u001b[0m\u001b[32m(BucketPositionBias) \u001b[0m\u001b[90mrelative_attention_bias:[4096]\u001b[0m\n",
      "\u001b[37mroot\u001b[0m\n",
      "├── \u001b[37mencoder \u001b[0m\u001b[32m(Encoder)\u001b[0m\n",
      "│   ├── \u001b[37mlayers \u001b[0m\u001b[32m(TransformerBlockList)\u001b[0m\n",
      "│   │   └── \u001b[31m0-47\u001b[0m\u001b[32m(CheckpointBlock)\u001b[0m\n",
      "│   │       ├── \u001b[37mself_att \u001b[0m\u001b[32m(SelfAttentionBlock)\u001b[0m\n",
      "│   │       │   ├── \u001b[37mlayernorm_before_attention \u001b[0m\u001b[32m(LayerNorm) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   │       │   └── \u001b[37mself_attention \u001b[0m\u001b[32m(Attention)\u001b[0m\n",
      "│   │       │       ├── \u001b[37mproject_q \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   │       │       │   └── \u001b[37mlora \u001b[0m\u001b[32m(DistributedLowRankLinear) \u001b[0m\u001b[35mlora_A:[8192] \u001b[0m\n",
      "│   │       │       │       \u001b[35mlora_B:[8192]\u001b[0m\n",
      "│   │       │       ├── \u001b[37mproject_k \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   │       │       ├── \u001b[37mproject_v \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[2099200]\u001b[0m\n",
      "│   │       │       │   └── \u001b[37mlora \u001b[0m\u001b[32m(DistributedLowRankLinear) \u001b[0m\u001b[35mlora_A:[8192] \u001b[0m\n",
      "│   │       │       │       \u001b[35mlora_B:[8192]\u001b[0m\n",
      "│   │       │       └── \u001b[37mattention_out \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[16777216]\u001b[0m\n",
      "│   │       └── \u001b[37mffn \u001b[0m\u001b[32m(FFNBlock)\u001b[0m\n",
      "│   │           ├── \u001b[37mlayernorm_before_ffn \u001b[0m\u001b[32m(LayerNorm) \u001b[0m\u001b[90mweight:[4096]\u001b[0m\n",
      "│   │           └── \u001b[37mffn \u001b[0m\u001b[32m(FeedForward)\u001b[0m\n",
      "│   │               ├── \u001b[37mw_in \u001b[0m\u001b[32m(DenseGatedACT)\u001b[0m\n",
      "│   │               │   ├── \u001b[37mw_0 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[29356032]\u001b[0m\n",
      "│   │               │   └── \u001b[37mw_1 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   │               └── \u001b[37mw_out \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   └── \u001b[37moutput_layernorm \u001b[0m\u001b[32m(LayerNorm) \u001b[0m\u001b[90mweight:[1024]\u001b[0m\n",
      "├── \u001b[37minput_embedding \u001b[0m\u001b[32m(EmbeddingExt) \u001b[0m\u001b[90mweight:[88660992]\u001b[0m\n",
      "└── \u001b[37mposition_bias \u001b[0m\u001b[32m(BucketPositionBias) \u001b[0m\u001b[90mrelative_attention_bias:[4096]\u001b[0m\n",
      "\u001b[37mroot\u001b[0m\n",
      "├── \u001b[37mencoder \u001b[0m\u001b[32m(Encoder)\u001b[0m\n",
      "│   ├── \u001b[37mlayers \u001b[0m\u001b[32m(TransformerBlockList)\u001b[0m\n",
      "│   │   └── \u001b[31m0-47\u001b[0m\u001b[32m(CheckpointBlock)\u001b[0m\n",
      "│   │       ├── \u001b[37mself_att \u001b[0m\u001b[32m(SelfAttentionBlock)\u001b[0m\n",
      "│   │       │   ├── \u001b[37mlayernorm_before_attention \u001b[0m\u001b[32m(LayerNorm) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   │       │   └── \u001b[37mself_attention \u001b[0m\u001b[32m(Attention)\u001b[0m\n",
      "│   │       │       ├── \u001b[31mproject_q,project_v\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   │       │       │   └── \u001b[37mlora \u001b[0m\u001b[32m(DistributedLowRankLinear) \u001b[0m\u001b[35mlora_A:[8192] \u001b[0m\n",
      "│   │       │       │       \u001b[35mlora_B:[8192]\u001b[0m\n",
      "│   │       │       └── \u001b[31mproject_k,attention_out\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   │       └── \u001b[37mffn \u001b[0m\u001b[32m(FFNBlock)\u001b[0m\n",
      "│   │           ├── \u001b[37mlayernorm_before_ffn \u001b[0m\u001b[32m(LayerNorm) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   │           └── \u001b[37mffn \u001b[0m\u001b[32m(FeedForward)\u001b[0m\n",
      "│   │               ├── \u001b[37mw_in \u001b[0m\u001b[32m(DenseGatedACT)\u001b[0m\n",
      "│   │               │   ├── \u001b[37mw_0 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   │               │   └── \u001b[37mw_1 \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[6293504]\u001b[0m\n",
      "│   │               └── \u001b[37mw_out \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[41943040]\u001b[0m\n",
      "│   └── \u001b[37moutput_layernorm \u001b[0m\u001b[32m(LayerNorm) \u001b[0m\u001b[90mweight:[1024]\u001b[0m\n",
      "├── \u001b[37minput_embedding \u001b[0m\u001b[32m(EmbeddingExt) \u001b[0m\u001b[90mweight:[88660992]\u001b[0m\n",
      "└── \u001b[37mposition_bias \u001b[0m\u001b[32m(BucketPositionBias) \u001b[0m\u001b[90mrelative_attention_bias:[4096]\u001b[0m\n",
      "[INFO|(OpenDelta)basemodel:696]2023-07-18 14:08:00,911 >> Trainable Ratio: 1572864/2405593088=0.065384%\n",
      "[INFO|(OpenDelta)basemodel:698]2023-07-18 14:08:00,911 >> Delta Parameter Ratio: 1572864/2405593088=0.065384%\n",
      "[INFO|(OpenDelta)basemodel:700]2023-07-18 14:08:00,911 >> Static Memory 4.48 GB, Max Memory 5.14 GB\n",
      "[INFO|(OpenDelta)basemodel:696]2023-07-18 14:08:00,917 >> Trainable Ratio: 1572864/2405593088=0.065384%\n",
      "[INFO|(OpenDelta)basemodel:698]2023-07-18 14:08:00,917 >> Delta Parameter Ratio: 1572864/2405593088=0.065384%\n",
      "[INFO|(OpenDelta)basemodel:700]2023-07-18 14:08:00,917 >> Static Memory 4.48 GB, Max Memory 5.14 GB\n",
      "[INFO|(OpenDelta)basemodel:696]2023-07-18 14:08:00,919 >> Trainable Ratio: 1572864/2405593088=0.065384%\n",
      "[INFO|(OpenDelta)basemodel:698]2023-07-18 14:08:00,919 >> Delta Parameter Ratio: 1572864/2405593088=0.065384%\n",
      "[INFO|(OpenDelta)basemodel:700]2023-07-18 14:08:00,919 >> Static Memory 4.48 GB, Max Memory 5.14 GB\n",
      "\u001b[37mroot\u001b[0m\n",
      "├── \u001b[37mencoder \u001b[0m\u001b[32m(Encoder)\u001b[0m\n",
      "│   ├── \u001b[37mlayers \u001b[0m\u001b[32m(TransformerBlockList)\u001b[0m\n",
      "│   │   └── \u001b[31m0-47\u001b[0m\u001b[32m(CheckpointBlock)\u001b[0m\n",
      "│   │       ├── \u001b[37mself_att \u001b[0m\u001b[32m(SelfAttentionBlock)\u001b[0m\n",
      "│   │       │   ├── \u001b[37mlayernorm_before_attention \u001b[0m\u001b[32m(LayerNorm) \u001b[0m\u001b[90mweight:[4096]\u001b[0m\n",
      "│   │       │   └── \u001b[37mself_attention \u001b[0m\u001b[32m(Attention)\u001b[0m\n",
      "│   │       │       ├── \u001b[37mproject_q \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[16777216]\u001b[0m\n",
      "│   │       │       │   └── \u001b[37mlora \u001b[0m\u001b[32m(DistributedLowRankLinear) \u001b[0m\u001b[35mlora_A:[8192] \u001b[0m\n",
      "│   │       │       │       \u001b[35mlora_B:[8192]\u001b[0m\n",
      "│   │       │       ├── \u001b[37mproject_k \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[16777216]\u001b[0m\n",
      "│   │       │       ├── \u001b[37mproject_v \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[14678016]\u001b[0m\n",
      "│   │       │       │   └── \u001b[37mlora \u001b[0m\u001b[32m(DistributedLowRankLinear) \u001b[0m\u001b[35mlora_A:[8192] \u001b[0m\n",
      "│   │       │       │       \u001b[35mlora_B:[8192]\u001b[0m\n",
      "│   │       │       └── \u001b[37mattention_out \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   │       └── \u001b[37mffn \u001b[0m\u001b[32m(FFNBlock)\u001b[0m\n",
      "│   │           ├── \u001b[37mlayernorm_before_ffn \u001b[0m\u001b[32m(LayerNorm) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   │           └── \u001b[37mffn \u001b[0m\u001b[32m(FeedForward)\u001b[0m\n",
      "│   │               ├── \u001b[37mw_in \u001b[0m\u001b[32m(DenseGatedACT)\u001b[0m\n",
      "│   │               │   └── \u001b[31mw_0,w_1\u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   │               └── \u001b[37mw_out \u001b[0m\u001b[32m(Linear) \u001b[0m\u001b[90mweight:[0]\u001b[0m\n",
      "│   └── \u001b[37moutput_layernorm \u001b[0m\u001b[32m(LayerNorm) \u001b[0m\u001b[90mweight:[1024]\u001b[0m\n",
      "├── \u001b[37minput_embedding \u001b[0m\u001b[32m(EmbeddingExt) \u001b[0m\u001b[90mweight:[88660992]\u001b[0m\n",
      "└── \u001b[37mposition_bias \u001b[0m\u001b[32m(BucketPositionBias) \u001b[0m\u001b[90mrelative_attention_bias:[4096]\u001b[0m\n",
      "[INFO|(OpenDelta)basemodel:696]2023-07-18 14:08:01,566 >> Trainable Ratio: 1572864/2405593088=0.065384%\n",
      "[INFO|(OpenDelta)basemodel:698]2023-07-18 14:08:01,566 >> Delta Parameter Ratio: 1572864/2405593088=0.065384%\n",
      "[INFO|(OpenDelta)basemodel:700]2023-07-18 14:08:01,566 >> Static Memory 4.48 GB, Max Memory 5.80 GB\n",
      "| Epoch:   1 | Iter:      1 | loss: 0.5699 | lr: 1.0000e-04, scale: 32768.0000 | time: 22.2446 | token/max: 0.9849 | mask/max: 0.0228 | grad_norm: 4.3164\n",
      "| task loss: 0.5699\n",
      "| Epoch:   1 | Iter:      2 | loss: 0.5763 | lr: 7.0711e-05, scale: 32768.0000 | time: 20.8937 | token/max: 0.9834 | mask/max: 0.0225 | grad_norm: 4.3169\n",
      "| task loss: 0.5763\n",
      "| Epoch:   1 | Iter:      3 | loss: 0.4988 | lr: 5.7735e-05, scale: 32768.0000 | time: 20.4068 | token/max: 0.9837 | mask/max: 0.0225 | grad_norm: 4.3736\n",
      "| task loss: 0.4988\n",
      "| Epoch:   1 | Iter:      4 | loss: 0.3788 | lr: 5.0000e-05, scale: 32768.0000 | time: 20.1719 | token/max: 0.9790 | mask/max: 0.0228 | grad_norm: 3.1356\n",
      "| task loss: 0.3788\n",
      "| Epoch:   1 | Iter:      5 | loss: 0.3357 | lr: 4.4721e-05, scale: 32768.0000 | time: 20.0452 | token/max: 0.9933 | mask/max: 0.0231 | grad_norm: 1.8761\n",
      "| task loss: 0.3357\n",
      "| Epoch:   1 | Iter:      6 | loss: 0.2254 | lr: 4.0825e-05, scale: 32768.0000 | time: 19.9471 | token/max: 0.9941 | mask/max: 0.0221 | grad_norm: 1.9085\n",
      "| task loss: 0.2254\n",
      "| Epoch:   1 | Iter:      7 | loss: 0.2219 | lr: 3.7796e-05, scale: 32768.0000 | time: 19.8852 | token/max: 0.9858 | mask/max: 0.0228 | grad_norm: 1.4707\n",
      "| task loss: 0.2219\n",
      "| Epoch:   1 | Iter:      8 | loss: 0.2175 | lr: 3.5355e-05, scale: 32768.0000 | time: 19.8375 | token/max: 0.9836 | mask/max: 0.0231 | grad_norm: 1.6504\n",
      "| task loss: 0.2175\n",
      "| Epoch:   1 | Iter:      9 | loss: 0.1718 | lr: 3.3333e-05, scale: 32768.0000 | time: 19.8016 | token/max: 0.9816 | mask/max: 0.0228 | grad_norm: 1.2911\n",
      "| task loss: 0.1718\n",
      "| Epoch:   1 | Iter:     10 | loss: 0.1540 | lr: 3.1623e-05, scale: 32768.0000 | time: 19.7693 | token/max: 0.9814 | mask/max: 0.0231 | grad_norm: 1.2423\n",
      "| task loss: 0.1540\n",
      "| Epoch:   1 | Iter:     11 | loss: 0.1859 | lr: 3.0151e-05, scale: 32768.0000 | time: 19.7483 | token/max: 0.9829 | mask/max: 0.0225 | grad_norm: 1.7779\n",
      "| task loss: 0.1859\n",
      "| Epoch:   1 | Iter:     12 | loss: 0.1873 | lr: 2.8868e-05, scale: 32768.0000 | time: 19.7304 | token/max: 0.9845 | mask/max: 0.0228 | grad_norm: 1.8277\n",
      "| task loss: 0.1873\n",
      "| Epoch:   1 | Iter:     13 | loss: 0.1516 | lr: 2.7735e-05, scale: 32768.0000 | time: 19.7130 | token/max: 0.9780 | mask/max: 0.0221 | grad_norm: 1.3050\n",
      "| task loss: 0.1516\n",
      "| Epoch:   1 | Iter:     14 | loss: 0.2008 | lr: 2.6726e-05, scale: 32768.0000 | time: 19.7016 | token/max: 0.9839 | mask/max: 0.0228 | grad_norm: 1.6280\n",
      "| task loss: 0.2008\n",
      "| Epoch:   1 | Iter:     15 | loss: 0.1652 | lr: 2.5820e-05, scale: 32768.0000 | time: 19.6903 | token/max: 0.9868 | mask/max: 0.0225 | grad_norm: 1.4054\n",
      "| task loss: 0.1652\n",
      "| Epoch:   1 | Iter:     16 | loss: 0.1327 | lr: 2.5000e-05, scale: 32768.0000 | time: 19.6772 | token/max: 0.9847 | mask/max: 0.0228 | grad_norm: 1.2544\n",
      "| task loss: 0.1327\n",
      "| Epoch:   1 | Iter:     17 | loss: 0.1332 | lr: 2.4254e-05, scale: 32768.0000 | time: 19.6723 | token/max: 0.9741 | mask/max: 0.0218 | grad_norm: 1.3962\n",
      "| task loss: 0.1332\n",
      "| Epoch:   1 | Iter:     18 | loss: 0.1497 | lr: 2.3570e-05, scale: 32768.0000 | time: 19.6639 | token/max: 0.9756 | mask/max: 0.0231 | grad_norm: 1.4654\n",
      "| task loss: 0.1497\n",
      "| Epoch:   1 | Iter:     19 | loss: 0.1232 | lr: 2.2942e-05, scale: 32768.0000 | time: 19.6555 | token/max: 0.9977 | mask/max: 0.0225 | grad_norm: 1.6121\n",
      "| task loss: 0.1232\n",
      "| Epoch:   1 | Iter:     20 | loss: 0.1167 | lr: 2.2361e-05, scale: 32768.0000 | time: 19.6491 | token/max: 0.9769 | mask/max: 0.0225 | grad_norm: 1.4358\n",
      "| task loss: 0.1167\n",
      "| Epoch:   1 | Iter:     21 | loss: 0.1105 | lr: 2.1822e-05, scale: 32768.0000 | time: 19.6426 | token/max: 0.9875 | mask/max: 0.0212 | grad_norm: 1.2782\n",
      "| task loss: 0.1105\n",
      "| Epoch:   1 | Iter:     22 | loss: 0.1627 | lr: 2.1320e-05, scale: 32768.0000 | time: 19.6374 | token/max: 0.9854 | mask/max: 0.0218 | grad_norm: 1.3677\n",
      "| task loss: 0.1627\n",
      "| Epoch:   1 | Iter:     23 | loss: 0.0975 | lr: 2.0851e-05, scale: 32768.0000 | time: 19.6352 | token/max: 0.9909 | mask/max: 0.0228 | grad_norm: 1.0773\n",
      "| task loss: 0.0975\n",
      "| Epoch:   1 | Iter:     24 | loss: 0.1663 | lr: 2.0412e-05, scale: 32768.0000 | time: 19.6334 | token/max: 0.9855 | mask/max: 0.0225 | grad_norm: 1.9180\n",
      "| task loss: 0.1663\n",
      "| Epoch:   1 | Iter:     25 | loss: 0.1155 | lr: 2.0000e-05, scale: 32768.0000 | time: 19.6306 | token/max: 0.9933 | mask/max: 0.0228 | grad_norm: 1.4684\n",
      "| task loss: 0.1155\n",
      "| Epoch:   1 | Iter:     26 | loss: 0.1277 | lr: 1.9612e-05, scale: 32768.0000 | time: 19.6292 | token/max: 0.9876 | mask/max: 0.0221 | grad_norm: 1.3680\n",
      "| task loss: 0.1277\n",
      "| Epoch:   1 | Iter:     27 | loss: 0.1469 | lr: 1.9245e-05, scale: 32768.0000 | time: 19.6264 | token/max: 0.9772 | mask/max: 0.0221 | grad_norm: 1.7380\n",
      "| task loss: 0.1469\n",
      "| Epoch:   1 | Iter:     28 | loss: 0.1109 | lr: 1.8898e-05, scale: 32768.0000 | time: 19.6253 | token/max: 0.9863 | mask/max: 0.0234 | grad_norm: 1.2173\n",
      "| task loss: 0.1109\n",
      "| Epoch:   1 | Iter:     29 | loss: 0.1232 | lr: 1.8570e-05, scale: 32768.0000 | time: 19.6248 | token/max: 0.9831 | mask/max: 0.0225 | grad_norm: 1.3683\n",
      "| task loss: 0.1232\n",
      "| Epoch:   1 | Iter:     30 | loss: 0.1317 | lr: 1.8257e-05, scale: 32768.0000 | time: 19.6216 | token/max: 0.9857 | mask/max: 0.0225 | grad_norm: 1.3746\n",
      "| task loss: 0.1317\n",
      "| Epoch:   1 | Iter:     31 | loss: 0.1025 | lr: 1.7961e-05, scale: 32768.0000 | time: 19.6213 | token/max: 0.9928 | mask/max: 0.0231 | grad_norm: 1.1562\n",
      "| task loss: 0.1025\n",
      "| Epoch:   1 | Iter:     32 | loss: 0.1220 | lr: 1.7678e-05, scale: 32768.0000 | time: 19.6187 | token/max: 0.9888 | mask/max: 0.0228 | grad_norm: 1.2197\n",
      "| task loss: 0.1220\n",
      "| Epoch:   1 | Iter:     33 | loss: 0.1214 | lr: 1.7408e-05, scale: 32768.0000 | time: 19.6180 | token/max: 0.9844 | mask/max: 0.0221 | grad_norm: 1.1574\n",
      "| task loss: 0.1214\n",
      "| Epoch:   1 | Iter:     34 | loss: 0.1129 | lr: 1.7150e-05, scale: 32768.0000 | time: 19.6152 | token/max: 0.9927 | mask/max: 0.0225 | grad_norm: 1.0373\n",
      "| task loss: 0.1129\n",
      "| Epoch:   1 | Iter:     35 | loss: 0.1201 | lr: 1.6903e-05, scale: 32768.0000 | time: 19.6151 | token/max: 0.9865 | mask/max: 0.0225 | grad_norm: 1.0397\n",
      "| task loss: 0.1201\n",
      "| Epoch:   1 | Iter:     36 | loss: 0.1181 | lr: 1.6667e-05, scale: 32768.0000 | time: 19.6132 | token/max: 0.9928 | mask/max: 0.0228 | grad_norm: 0.9622\n",
      "| task loss: 0.1181\n",
      "| Epoch:   1 | Iter:     37 | loss: 0.1020 | lr: 1.6440e-05, scale: 32768.0000 | time: 19.6169 | token/max: 0.9904 | mask/max: 0.0228 | grad_norm: 1.0426\n",
      "| task loss: 0.1020\n",
      "| Epoch:   1 | Iter:     38 | loss: 0.1018 | lr: 1.6222e-05, scale: 32768.0000 | time: 19.6164 | token/max: 0.9855 | mask/max: 0.0218 | grad_norm: 0.9116\n",
      "| task loss: 0.1018\n",
      "| Epoch:   1 | Iter:     39 | loss: 0.1062 | lr: 1.6013e-05, scale: 32768.0000 | time: 19.6131 | token/max: 0.9823 | mask/max: 0.0225 | grad_norm: 1.0089\n",
      "| task loss: 0.1062\n",
      "| Epoch:   1 | Iter:     40 | loss: 0.0712 | lr: 1.5811e-05, scale: 32768.0000 | time: 19.6117 | token/max: 0.9793 | mask/max: 0.0225 | grad_norm: 0.9411\n",
      "| task loss: 0.0712\n",
      "| Epoch:   1 | Iter:     41 | loss: 0.1122 | lr: 1.5617e-05, scale: 32768.0000 | time: 19.6099 | token/max: 0.9847 | mask/max: 0.0231 | grad_norm: 0.8777\n",
      "| task loss: 0.1122\n",
      "| Epoch:   1 | Iter:     42 | loss: 0.1093 | lr: 1.5430e-05, scale: 32768.0000 | time: 19.6076 | token/max: 0.9922 | mask/max: 0.0221 | grad_norm: 1.0416\n",
      "| task loss: 0.1093\n",
      "| Epoch:   1 | Iter:     43 | loss: 0.1149 | lr: 1.5250e-05, scale: 32768.0000 | time: 19.6086 | token/max: 0.9800 | mask/max: 0.0225 | grad_norm: 1.2296\n",
      "| task loss: 0.1149\n",
      "| Epoch:   1 | Iter:     44 | loss: 0.1027 | lr: 1.5076e-05, scale: 32768.0000 | time: 19.6091 | token/max: 0.9827 | mask/max: 0.0225 | grad_norm: 0.9353\n",
      "| task loss: 0.1027\n",
      "| Epoch:   1 | Iter:     45 | loss: 0.0829 | lr: 1.4907e-05, scale: 32768.0000 | time: 19.6087 | token/max: 0.9873 | mask/max: 0.0225 | grad_norm: 1.0207\n",
      "| task loss: 0.0829\n",
      "| Epoch:   1 | Iter:     46 | loss: 0.1100 | lr: 1.4744e-05, scale: 32768.0000 | time: 19.6339 | token/max: 0.9854 | mask/max: 0.0228 | grad_norm: 0.9511\n",
      "| task loss: 0.1100\n",
      "| Epoch:   1 | Iter:     47 | loss: 0.1323 | lr: 1.4586e-05, scale: 32768.0000 | time: 19.6358 | token/max: 0.9881 | mask/max: 0.0228 | grad_norm: 1.1391\n",
      "| task loss: 0.1323\n",
      "| Epoch:   1 | Iter:     48 | loss: 0.1220 | lr: 1.4434e-05, scale: 32768.0000 | time: 19.6312 | token/max: 0.9915 | mask/max: 0.0228 | grad_norm: 0.9877\n",
      "| task loss: 0.1220\n",
      "| Epoch:   1 | Iter:     49 | loss: 0.0763 | lr: 1.4286e-05, scale: 32768.0000 | time: 19.6294 | token/max: 0.9740 | mask/max: 0.0215 | grad_norm: 0.9290\n",
      "| task loss: 0.0763\n",
      "| Epoch:   1 | Iter:     50 | loss: 0.1011 | lr: 1.4142e-05, scale: 32768.0000 | time: 19.6290 | token/max: 0.9857 | mask/max: 0.0225 | grad_norm: 1.1550\n",
      "| task loss: 0.1011\n",
      "evaluation begins...\n",
      "| Eval loss: 0.0946 | Increase:  0\n",
      "| Epoch:   1 | Iter:     51 | loss: 0.1128 | lr: 1.4003e-05, scale: 32768.0000 | time: 19.6279 | token/max: 0.9875 | mask/max: 0.0228 | grad_norm: 0.9297\n",
      "| task loss: 0.1128\n",
      "| Epoch:   1 | Iter:     52 | loss: 0.1028 | lr: 1.3868e-05, scale: 32768.0000 | time: 19.6263 | token/max: 0.9915 | mask/max: 0.0228 | grad_norm: 0.9309\n",
      "| task loss: 0.1028\n",
      "| Epoch:   1 | Iter:     53 | loss: 0.0992 | lr: 1.3736e-05, scale: 32768.0000 | time: 19.6266 | token/max: 0.9886 | mask/max: 0.0221 | grad_norm: 0.9702\n",
      "| task loss: 0.0992\n",
      "| Epoch:   1 | Iter:     54 | loss: 0.1138 | lr: 1.3608e-05, scale: 32768.0000 | time: 19.6223 | token/max: 0.9850 | mask/max: 0.0225 | grad_norm: 1.1470\n",
      "| task loss: 0.1138\n",
      "| Epoch:   1 | Iter:     55 | loss: 0.1381 | lr: 1.3484e-05, scale: 32768.0000 | time: 19.6200 | token/max: 0.9912 | mask/max: 0.0225 | grad_norm: 1.2395\n",
      "| task loss: 0.1381\n",
      "| Epoch:   1 | Iter:     56 | loss: 0.0861 | lr: 1.3363e-05, scale: 32768.0000 | time: 19.6203 | token/max: 0.9797 | mask/max: 0.0221 | grad_norm: 1.1209\n",
      "| task loss: 0.0861\n",
      "| Epoch:   1 | Iter:     57 | loss: 0.0826 | lr: 1.3245e-05, scale: 32768.0000 | time: 19.6188 | token/max: 0.9912 | mask/max: 0.0218 | grad_norm: 1.0277\n",
      "| task loss: 0.0826\n",
      "| Epoch:   1 | Iter:     58 | loss: 0.0930 | lr: 1.3131e-05, scale: 32768.0000 | time: 19.6177 | token/max: 0.9837 | mask/max: 0.0228 | grad_norm: 0.9935\n",
      "| task loss: 0.0930\n",
      "| Epoch:   1 | Iter:     59 | loss: 0.0755 | lr: 1.3019e-05, scale: 32768.0000 | time: 19.6197 | token/max: 0.9790 | mask/max: 0.0231 | grad_norm: 1.0600\n",
      "| task loss: 0.0755\n",
      "| Epoch:   1 | Iter:     60 | loss: 0.1125 | lr: 1.2910e-05, scale: 32768.0000 | time: 19.6164 | token/max: 0.9863 | mask/max: 0.0228 | grad_norm: 1.0935\n",
      "| task loss: 0.1125\n",
      "| Epoch:   1 | Iter:     61 | loss: 0.0961 | lr: 1.2804e-05, scale: 32768.0000 | time: 19.6141 | token/max: 0.9868 | mask/max: 0.0221 | grad_norm: 1.0253\n",
      "| task loss: 0.0961\n",
      "| Epoch:   1 | Iter:     62 | loss: 0.0870 | lr: 1.2700e-05, scale: 32768.0000 | time: 19.6133 | token/max: 0.9891 | mask/max: 0.0225 | grad_norm: 0.9059\n",
      "| task loss: 0.0870\n",
      "| Epoch:   1 | Iter:     63 | loss: 0.0930 | lr: 1.2599e-05, scale: 32768.0000 | time: 19.6147 | token/max: 0.9886 | mask/max: 0.0231 | grad_norm: 0.9990\n",
      "| task loss: 0.0930\n",
      "| Epoch:   1 | Iter:     64 | loss: 0.0981 | lr: 1.2500e-05, scale: 32768.0000 | time: 19.6114 | token/max: 0.9884 | mask/max: 0.0241 | grad_norm: 1.0210\n",
      "| task loss: 0.0981\n",
      "| Epoch:   1 | Iter:     65 | loss: 0.1072 | lr: 1.2403e-05, scale: 32768.0000 | time: 19.6166 | token/max: 0.9746 | mask/max: 0.0225 | grad_norm: 1.2010\n",
      "| task loss: 0.1072\n",
      "| Epoch:   1 | Iter:     66 | loss: 0.0809 | lr: 1.2309e-05, scale: 32768.0000 | time: 19.6161 | token/max: 0.9823 | mask/max: 0.0225 | grad_norm: 0.9356\n",
      "| task loss: 0.0809\n",
      "| Epoch:   1 | Iter:     67 | loss: 0.1460 | lr: 1.2217e-05, scale: 32768.0000 | time: 19.6157 | token/max: 0.9899 | mask/max: 0.0231 | grad_norm: 1.5303\n",
      "| task loss: 0.1460\n",
      "| Epoch:   1 | Iter:     68 | loss: 0.0746 | lr: 1.2127e-05, scale: 32768.0000 | time: 19.6143 | token/max: 0.9899 | mask/max: 0.0225 | grad_norm: 0.9893\n",
      "| task loss: 0.0746\n",
      "| Epoch:   1 | Iter:     69 | loss: 0.1276 | lr: 1.2039e-05, scale: 32768.0000 | time: 19.6121 | token/max: 0.9938 | mask/max: 0.0231 | grad_norm: 1.3122\n",
      "| task loss: 0.1276\n",
      "| Epoch:   1 | Iter:     70 | loss: 0.1291 | lr: 1.1952e-05, scale: 32768.0000 | time: 19.6106 | token/max: 0.9827 | mask/max: 0.0221 | grad_norm: 1.3542\n",
      "| task loss: 0.1291\n",
      "| Epoch:   1 | Iter:     71 | loss: 0.1215 | lr: 1.1868e-05, scale: 32768.0000 | time: 19.6140 | token/max: 0.9842 | mask/max: 0.0218 | grad_norm: 1.3603\n",
      "| task loss: 0.1215\n",
      "| Epoch:   1 | Iter:     72 | loss: 0.0925 | lr: 1.1785e-05, scale: 32768.0000 | time: 19.6145 | token/max: 0.9919 | mask/max: 0.0221 | grad_norm: 1.2080\n",
      "| task loss: 0.0925\n",
      "| Epoch:   1 | Iter:     73 | loss: 0.1053 | lr: 1.1704e-05, scale: 32768.0000 | time: 19.6102 | token/max: 0.9858 | mask/max: 0.0228 | grad_norm: 0.9347\n",
      "| task loss: 0.1053\n",
      "| Epoch:   1 | Iter:     74 | loss: 0.1025 | lr: 1.1625e-05, scale: 32768.0000 | time: 19.6113 | token/max: 0.9979 | mask/max: 0.0228 | grad_norm: 1.0367\n",
      "| task loss: 0.1025\n",
      "| Epoch:   1 | Iter:     75 | loss: 0.1082 | lr: 1.1547e-05, scale: 32768.0000 | time: 19.6117 | token/max: 0.9932 | mask/max: 0.0231 | grad_norm: 1.2256\n",
      "| task loss: 0.1082\n",
      "| Epoch:   1 | Iter:     76 | loss: 0.0912 | lr: 1.1471e-05, scale: 32768.0000 | time: 19.6097 | token/max: 0.9759 | mask/max: 0.0225 | grad_norm: 0.9999\n",
      "| task loss: 0.0912\n",
      "| Epoch:   1 | Iter:     77 | loss: 0.1227 | lr: 1.1396e-05, scale: 32768.0000 | time: 19.6113 | token/max: 0.9837 | mask/max: 0.0225 | grad_norm: 1.1791\n",
      "| task loss: 0.1227\n",
      "| Epoch:   1 | Iter:     78 | loss: 0.0749 | lr: 1.1323e-05, scale: 32768.0000 | time: 19.6085 | token/max: 0.9881 | mask/max: 0.0221 | grad_norm: 1.3797\n",
      "| task loss: 0.0749\n",
      "| Epoch:   1 | Iter:     79 | loss: 0.0696 | lr: 1.1251e-05, scale: 32768.0000 | time: 19.6091 | token/max: 0.9810 | mask/max: 0.0228 | grad_norm: 1.1065\n",
      "| task loss: 0.0696\n"
     ]
    }
   ],
   "source": [
    "!sh CPM-Bee/src/scripts/finetune_cpm_bee.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3766012",
   "metadata": {},
   "source": [
    "您可以在`results/`中查看存储的模型或者轻量级 Delta 模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7257feb-bcb4-4952-97d4-c6c7f13a6151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p38",
   "language": "python",
   "name": "conda_mxnet_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
